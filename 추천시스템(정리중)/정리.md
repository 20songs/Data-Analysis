# YES24_도서추천모델개발

## 0. 전처리

#### 0.1 파일 읽기 (JSON)

| 파일 이름    | 내용                             | 크기         |
| ------------ | -------------------------------- | ------------ |
| Click_Stream | 유저의 도서 클릭 정보 로그데이터 | (24105214,5) |
| Accounts     | 유저의 demography 정보           | (1741578,6)  |
| Products     | 도서의 상품 정보                 | (1745066,6)  |
| Orders       | 유저의 도서 구매 정보            | (8382514,5)  |

##### 사용한 코드

> 각 4가지 json 파일에 대해 해당 작업을 수행하여 csv 파일로 만들었음

```python
import json
import os
from os import listdir
from os.path import isfile, join
import pandas as pd

path = '경로'

# files: 디렉토리 안의 파일 목록을 리스트로 만듭니다.
files = [f for f in listdir('디렉토리 경로') if isfile(join('디렉토리 경로', f))]

# json_files: 파일 목록을 파일 이름으로 하여 DataFrame을 만듭니다
json_files = pd.DataFrame({"file_id" : files})

# 초기 dataframe 칼럼을 선언해주기 위해 0번째 값을 초기화 시킵니다.
with open('디렉토리 경로'+json_files['file_id'][0], encoding = 'utf-8') as f:
        DF = pd.DataFrame(json.loads(line) for line in f)

# json file 목록을 읽으며 각 파일의 json 형태로 저장된 txt 파일을 DataFrame으로 형변환 한 후 concat를 통해 전체 데이터프레임 형성합니다.
for i in range(1,len(json_files)):
    with open('디렉토리 경로'+json_files['file_id'][i], encoding = 'utf-8') as f:
        tmp = pd.DataFrame(json.loads(line) for line in f)
    DF = pd.concat([DF, tmp])
```



#### 0.2 데이터 별 전처리

##### Accounts

| 칼럼           | 설명                          | 자료형  |
| -------------- | ----------------------------- | ------- |
| accounts_id    | 유저의 아이디 (6자리 숫자)    | int64   |
| gender         | 성별 (F/M)                    | object  |
| age            | 나이 (연속형)                 | float64 |
| address        | 주소 (시/도  -  군/구  -  동) | object  |
| zip_code       | 우편번호                      | object  |
| last_login_dts | json time                     | int64   |

* 결측치 및 이상치 처리

  > data가 매우 많기 때문에...삭제합니다

  * dropna로 일괄 삭제
  * age 중 7세 이하 삭제
  * 성별 중 F/M이 아닌 모든 값 삭제

* 접속 일자[last_login_dts]

  * date_time 형식으로 변환
  * `*1000000`을 통해 우리가 알 수 있는 형식으로 값을 변환

* 성별[gender]

  * Label Encoding
  * M = 1, F=0

* 나이[Age]

  * Categorical Encoding
    * 학령기 등을 기준으로 나이를 나눔
    * ~ 12세 / 12~19세 / 20~29세 / 30~40세 / 40~50세 / 51~64세 / 65세 이상
    * 각각 초등학생 / 중,고등학생 / 대학,취준생 / 사회초년생 / 자녀를 둔 부모 / 퇴직 전 / 퇴직 후

* 주소

  * 수도권 / 비수도권 더미 인코딩
    * 인천, 경기, 서울 = 수도권

##### Orders

| 칼럼       | 설명                                    | 자료형  |
| ---------- | --------------------------------------- | ------- |
| order_id   | 주문 시 생성되는 id 값                  | int64   |
| account_id | 주문한 유저의 id 값                     | int64   |
| product_id | 주문된 도서의 id 값                     | int64   |
| price      | 할인 등 기타 사항이 반영된 최종 결제 값 | float64 |
| created_at | java time                               | int64   |

* 결측치 삭제
* created_at
  * 주문 시간의 경우 java_time 변환
    * 초 = java_time / 1000
    * datetime.datetime.fromtimestamp(java_time/1000)
    * Y-m-d 형식으로 변환

##### Products

| 칼럼         | 설명           | 자료형    |
| ------------ | -------------- | --------- |
| product_id   | 도서의 id 값   | int64     |
| product_name | 도서 제목      | object    |
| category_id  | 도서 분류 id값 | int64     |
| published_at | 도서 출판 일   | date-time |
| shop_price   | 도서 정가      | float64   |
| maker_name   | 출판사 이름    | object    |

* 결측치 삭제
* 출판일
  * 이상치 확인하여 삭제
    * 8자리가 되지 않거나
    * 13월로 표기되거나
    * 0인 값
  * `from dateutil.parser import parse`
    * parse기능을 사용하여 하이픈(-) 형식으로 변환
  * Categorical Encoding
    * 3개월 미만 / 3개월 ~ 6개월 / 6개월 ~ 1년 / 1년 ~ 3년 / 3년 이후
* 가격
  * 배송 가능한 가격, 할인 가능한 가격, 전공 서적의 가격 등의 정보를 반영
    * 10,000원 미만 / 10,000원 ~ 14,000원 / 14,000원 ~ 19,000원 / 19,000원 이상
* 도서 카테고리
  * 일련 체계 상 뒷 3자리가 동일한 층위에 해당함
    * 인덱싱을 통해 처리 `[1:]`

##### Click

| 칼럼              | 설명                      | 자료형    |
| ----------------- | ------------------------- | --------- |
| request_date_time | 클릭 이벤트가 발생한 시간 | date-time |
| acount_id         | 유저 id                   | int64     |
| device_type       | 접속한 기기 정보 [M/P]    | object    |
| product_id        | 도서 id                   | int64     |
| before_product_id | 직전에 클릭한 도서 id     | int64     |

* Device type
  * 라벨 인코딩
  * Mobile:0, PC:1

#### 0.3 Feature Engineer

* orders & clicks

  * 제공받은accounts, producdts 데이터가 click을 기준으로 하였기 때문에
  * orders 중 clicks에 없는 정보는 삭제하였음
  * YES24가 아닌 외부에서 구매된 상품인 경우에 해당함

* New Preference

  > 유저가 얼마나 신간 도서를 좋아하는가?

  * Click 데이터 기준, Product 데이터 결합
  * 2020년을 기준으로 신간 / 비신간 도서 구분
  * groupby method를 통해 account_id 별 신간 / 비신간 횟수 계산
  * Click 데이터의 account_id가 신간을 클릭한 횟수 / 비신간을 클릭한 횟수를 비율 계산
  * Account_id 별로 New Preference 선호 정도를 구할 수 있음

* Category Preference

  > 유저가 어떤 도서의 종류를 좋아하는가?

  * Click 데이터 기준, Product 데이터 결합
  * category 정보를 dummy coding 하여 칼럼 확장 [One-hot encoding]
  * Group by를 이용하여 account_id 별 category 클릭 횟수를 누적 (sum)
  * 그 중 가장 max인 cateogry 정보와 해당 클릭 횟수를 feature로 삼음

* 활동 시간

  > 유저는 낮,밤 어느 시간 대를 선호하는가?

  * Click 데이터의 request_date_time을 이용
  * 한 개별 유저라도 낮 / 밤에 따라 선호하는 취향이 다를 것으로 가정함
  * 퇴근시간 7시를 기준으로 day / night을 나눔 [Onehot Encoding]
  * GROUPBY method를 이용하며 접속 시간을 day / night으로 더미 인코딩하여 account_id 별로 클릭 횟수를 누적(sum)
  * 비율을 계산하여 feature로 삼음

* 활동 요일

  > 유저는 평일, 주말 중 어느 요일을 선호하는가?

  * Click 데이터의 request_date_time을 이용
  * 한 개별 유저라도 평일 / 주말에 따라 취향이 달라질 것으로 가정함
  * 월~목 = 평일 / 금~일 = 주말 로 Onehot Encoding
  * GROUPBY METHOD를 이용하여 Account_ID 별 평일/주말 클릭 횟수를 누적(sum)
  * 비율을 계산하여 각각 feature로 삼음

* 접속 기기

  > 유저가 어떤 기기를 선호하는가?

  * Click 데이터의 device를 이용
  * 한 개별 유저라도 접속하는 기기에 따라 선호하는 취향이 달라질 것으로 가정
  * GROUPBY Method를 이용하여 접속 기기 별 클릭 횟수를 누적(sum)
  * 비율을 계산하여 각각 feature로 삼음

* 관여도

  > 유저가 구매에 대해 얼마나 신경쓰는가?

  * Click 데이터와 Order 데이터의 차이를 이용
  * Click 횟수에 따라 Order 횟수를 나누어 비율 Feature를 생성
  * Order횟수를 Click횟수로 나누어 구매당 클릭 비율을 생성하여 Feature로 선정

* BestSeller

  > 도서의 베스트 셀러 유무가 유저의 구매에 영향을 미치는가?

  * 새로운 BestSeller 정보가 담긴 데이터를 요청
  * Click 기준으로 Best Seller 를 클릭한 횟수를 누적
  * 총 클릭 수에 대한 Best Seller 클릭 수를 비율 값으로 Feature를 선정

#### 0.4 전처리 결과물 [Dummy 데이터 - 향후 결합]

* User dummy
  * Accounts_id
  * Gender
    * Gender 0: 남성
    * Gender 1: 여성
  * Age
    * Age 0 : 12세 이하
    * ..
    * Age 6: 65세 이상
  * Category Preference
    * pref 1: 수험서 자격증
    * ...
    * pref 34: 에세이
  * New Preference
    * pref 0: 올해(2020)
    * pref 1: 과거(2020년 이전)
  * Address
    * Address 0:  수도권
    * Address 1: 비수도권
* Book dummy
  * Product_id
  * Category
    * cat 1: 수험서 자격증
    * ...
    * cat 34: 에세이
  * Published date
    * pub 0: 3개월 미만
    * ...
    * pub 4: 3년 이후
  * Price
    * price 0: 10,000원 미만
    * ...
    * Price 3: 19,000원 이상

## 1. Context Vector

#### 1.1 K-MEANS

* Clustered data
  * User Cluster: 6
  * Book Cluster: 5
  * 필드 구성
    * User account - Book product - User Cluster - Book  Cluster - Purchased

#### 1.2 정규화

* User / Product 칼럼에 대한 정규화
  * K-means 기준으로 Aggregate
  * 정규화



* 발표 대본

  모델 설명에 앞서 모델의 인풋값으로 들어간 데이터의 형태에 대해서 잠시 설명드리자면, User와 Book Feature를 이와 같이 더미변수화하였고, 더미화로 인해 생기는 sparse matrix문제를 정규화를 통해 해결하였습니다.
  그리고 개별 책과 개별 사람을 고려하는 데는 너무나 방대한 연산과 cold-start문제가 발생하기 때문에, book feature와 user feature를 가지고 k-means를 통해 각각 book cluster와 user cluster를 이와 같이 만들었습니다.

## 2. 모델링

* 수도 코드

  * 5 그룹의 책 cluster 별로 context feature에 대한 prior distribution 설정
  * 새로운 context 발생 시 현재 distribution에서 coefficient sampling
  * Context 와 sampling 결과 값을 계산하여 가장 높은 scalar 값을 나타내는 책 Cluster 선택
  * 선택된 책 

* 발표 대본 설명

  먼저 책 cluster별로 각 context의 prior distribution을 가정한 후, 새로운 context가 발생 시 현재까지의 distribution에서 sampling 합니다. 그리고 context와 sampling 결과 값을 계산하여 가장 높은 scalar값을 나타내는 책 cluster를 선택합니다. 마지막으로 선택된 책 cluster의 reward값이 0인지, 1인지를 관측 후 결과에 따라 해당 책의 cluster의 distribution을 업데이트 하며 점차 개인맞춤형 책 cluster를 추천하는 방향으로 나아가는 모델입니다.

## 3.평가

* off-policy

  * 임의의 책 1권 선정 ["내가 원하는 것을 나도 모를 때"]
    * 해당 책을 Click한 User 중 Random User 100 명 선정
    * 이에 따른 24권의 도서 선정

* T/S 적용 1번 방식

  첫번째 톰슨 샘플링 모델의 경우 user 100명을 선정하는 것까지는 YES24와 동일한 상황을 가정하고, 

  유저 클러스터에 대한 책 cluster의 구매 여부에 따른 베타 분포를 학습합니다. 

  그리고 click event가 발생 시 유저 cluster와 책 cluster의 확률 분포에서 임의의 값을 sampling합니다. 

  그리고 해당 user cluster에 대해 가장 큰 sampling 값을 갖는 책 cluster를 선택합니다. 

  그 후 선택된 책 cluster의 책들에 대해 학습한 context의 coeff를 내적하여 유저별 상위 24개를 추출합니다.

   마지막으로 실제 사용자가 구매한 책 중 이 모델이 추천한 책이 포함된 값을 계산합니다.

* T/S 적용 2번 방식

  두번째 톰슨 샘플링 모델의 경우 아까와 같이 user 100명 선정하는 단계까지는 동일하며, 

  첫번째 모델처럼 train data에서 책 cluster별 context의 coeff를 학습하는데, 

  다만 선택된 책의 pool 내에서만 뽑는 것이 아니라 train data에 있는 전체 책에 대해 학습합니다. 

  test data에서는 click event 발생시 학습된 feature별 확률 분포에서 임의 값을 sampling하고, 

  sampling한 값을 click event마다 생기는 context feature와 내적하여 가장 큰 값을 갖는 유저 당 상위 24권을 추출합니다.